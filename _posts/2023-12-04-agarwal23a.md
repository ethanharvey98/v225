---
title: Representing visual classification as a linear combination of words
abstract: Explainability is a longstanding challenge in deep learning, especially
  in high-stakes domains like healthcare. Common explainability methods highlight
  image regions that drive an AI model{’}s decision. Humans, however, heavily rely
  on language to convey explanations of not only “where” but {“}what{”}. Additionally,
  most explainability approaches focus on explaining individual AI predictions, rather
  than describing the features used by an AI model in general. The latter would be
  especially useful for model and dataset auditing, and potentially even knowledge
  generation as AI is increasingly being used in novel tasks. Here, we present an
  explainability strategy that uses a vision-language model to identify language-based
  descriptors of a visual classification task. By leveraging a pre-trained joint embedding
  space between images and text, our approach estimates a new classification task
  as a linear combination of words, resulting in a weight for each word that indicates
  its alignment with the vision-based classifier. We assess our approach using two
  medical imaging classification tasks, where we find that the resulting descriptors
  largely align with clinical knowledge despite a lack of domain-specific language
  training. However, our approach also identifies the potential for {‘}shortcut connections{’}
  in the public datasets used. Towards a functional measure of explainability, we
  perform a pilot reader study where we find that the AI-identified words can enable
  non-expert humans to perform a specialized medical task at a non-trivial level.
  Altogether, our results emphasize the potential of using multimodal foundational
  models to deliver intuitive, language-based explanations of visual tasks.
software: https://github.com/lotterlab/task_word_explainability
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: agarwal23a
month: 0
tex_title: Representing visual classification as a linear combination of words
firstpage: 27
lastpage: 38
page: 27-38
order: 27
cycles: false
bibtex_author: Agarwal, Shobhit and Semenov, Yevgeniy R. and Lotter, William
author:
- given: Shobhit
  family: Agarwal
- given: Yevgeniy R.
  family: Semenov
- given: William
  family: Lotter
date: 2023-12-04
address: 
container-title: Proceedings of the 3rd Machine Learning for Health Symposium
volume: '225'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 12
  - 4
pdf: https://proceedings.mlr.press/v225/agarwal23a/agarwal23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
